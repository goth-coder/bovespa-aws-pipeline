# ğŸ“Œ Instructions for Copilot Agent (Claude Sonnet 4)

## ğŸ— Contexto do Projeto
Estamos desenvolvendo um **Pipeline de Dados Batch** para ingestÃ£o, processamento e anÃ¡lise de dados do pregÃ£o da **B3**.  
O projeto faz parte do **Tech Challenge - Big Data Architecture** e serÃ¡ implementado utilizando serviÃ§os da **AWS** e **Python**.

---

## ğŸ¯ Objetivo
Construir uma arquitetura que:
1. **Coleta dados** do site da B3 via scraping.
2. **Armazena** no **AWS S3** (Data Lake) em formato **Parquet** com partiÃ§Ã£o diÃ¡ria.
3. **Orquestra** o processamento via **AWS Lambda**, que dispara **AWS Glue**.
4. **Processa e transforma** os dados no **AWS Glue** (modo visual) aplicando as transformaÃ§Ãµes obrigatÃ³rias:
   - Agrupamento numÃ©rico (sumarizaÃ§Ã£o, contagem ou soma)
   - Renomear 2 colunas
   - CÃ¡lculo com campos de data
5. **Salva dados refinados** no S3 (`/refined/`) em Parquet, particionados por **data** e **nome/abreviaÃ§Ã£o da aÃ§Ã£o**.
6. **CatalogaÃ§Ã£o automÃ¡tica** no Glue Catalog.
7. **Consulta** dos dados no AWS Athena.

---

## ğŸ›  Tecnologias e Ferramentas
- **Python 3.10+**
- **AWS S3**
- **AWS Lambda**
- **AWS Glue**
- **AWS Athena**
- **Parquet**
- **Boto3** (SDK AWS para Python)
- **Pandas** (processamento local de dados)
- **Requests / BeautifulSoup** (scraping)

---

## ğŸ“‹ Estrutura do Projeto
```bash
.
â”œâ”€â”€ README.md
â”œâ”€â”€ instructions.md           # Este arquivo
â”œâ”€â”€ scraping/                 # Scripts de scraping da B3
â”œâ”€â”€ lambda/                   # CÃ³digo da funÃ§Ã£o AWS Lambda
â”œâ”€â”€ glue/                     # Scripts auxiliares e configs do Job Glue
â”œâ”€â”€ docs/                     # Diagramas e documentaÃ§Ã£o 
```

---

## ğŸ“‹ Requisitos Funcionais
- [ ] Criar script de **scraping** para coletar dados do pregÃ£o da B3 (`https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br`)
- [ ] Converter dados para **Parquet**
- [ ] Adicionar **partiÃ§Ã£o diÃ¡ria** ao salvar
- [ ] Criar bucket S3 com pastas `/raw/` e `/refined/`
- [ ] Configurar upload automÃ¡tico do scraping para `/raw/`
- [ ] Criar funÃ§Ã£o Lambda para acionar Job Glue
- [ ] Configurar gatilho S3 â†’ Lambda
- [ ] Criar Job Glue (modo visual) com as transformaÃ§Ãµes obrigatÃ³rias
- [ ] Salvar dados refinados particionados por data e nome da aÃ§Ã£o
- [ ] Catalogar dados no Glue Catalog
- [ ] Validar consultas no AWS Athena
- [ ] (Opcional) Criar Notebook no Athena com visualizaÃ§Ã£o grÃ¡fica

---

## ğŸ“œ Guidelines para o Copilot Agent
1. **Sempre consultar o arquivo `README.md`** antes de gerar qualquer cÃ³digo ou documentaÃ§Ã£o, pois ele contÃ©m o checklist atualizado e detalhes do projeto.
2. **Gerar cÃ³digo Python** sempre compatÃ­vel com **3.10+**.
3. Ao criar cÃ³digo AWS, **usar Boto3** e boas prÃ¡ticas de seguranÃ§a (variÃ¡veis de ambiente, IAM roles).
4. **Sempre** documentar funÃ§Ãµes com docstrings claras.
5. Quando gerar scripts AWS Lambda:
   - Otimizar para **tempo de execuÃ§Ã£o baixo**.
   - Garantir que a funÃ§Ã£o **apenas dispare** o Glue Job (nÃ£o processa dados).
6. Para o scraping:
   - Usar **Requests** + **BeautifulSoup**.
   - Garantir tolerÃ¢ncia a falhas (try/except, logs).
7. Para integraÃ§Ã£o com S3:
   - Usar **upload_fileobj** ou **put_object** do Boto3.
   - Nomear arquivos com padrÃ£o: `YYYY-MM-DD.parquet` e path `/raw/dt=YYYY-MM-DD/`.
8. Para Glue:
   - Retornar exemplos de configuraÃ§Ã£o no **modo visual**.
   - Garantir as trÃªs transformaÃ§Ãµes obrigatÃ³rias.
9. Para Athena:
   - Fornecer queries de exemplo para validaÃ§Ã£o.
10. Sempre manter consistÃªncia com a **arquitetura de pipeline definida**:
   ```plaintext
   Scraping â†’ S3 (raw) â†’ Lambda â†’ Glue â†’ S3 (refined) â†’ Glue Catalog â†’ Athena
   ```
11. Quando possÃ­vel, fornecer **exemplos de teste unitÃ¡rio** para cada parte.

---

## ğŸ“Š Fluxo Esperado
```plaintext
1. Scraping coleta dados da B3
2. Salva no S3 `/raw/` (Parquet, partiÃ§Ã£o diÃ¡ria)
3. Evento S3 aciona Lambda
4. Lambda dispara Glue Job
5. Glue processa dados e salva no `/refined/`
6. Glue Catalog atualiza tabela
7. Athena consulta dados refinados
```

---

## âœ… Objetivo Final
Ter um pipeline 100% funcional que:
- Coleta, processa e armazena dados da B3.
- Ã‰ escalÃ¡vel e pronto para uso em produÃ§Ã£o.
- EstÃ¡ documentado com README e diagramas.
- Possui scripts reprodutÃ­veis para cada etapa.
