# ğŸš€ Pipeline Batch Bovespa - AWS

## ğŸ“œ Sobre o Projeto
Este projeto implementa um **pipeline de dados batch** completo para ingestÃ£o, processamento e anÃ¡lise dos dados do pregÃ£o da **B3 (Bolsa de Valores de SÃ£o Paulo)**, utilizando serviÃ§os da **AWS**.

O pipeline executa as seguintes etapas:
1. **Scraping** automatizado dos dados da B3 via API
2. **TransformaÃ§Ã£o** para formato Parquet otimizado
3. **Armazenamento** no S3 com particionamento por data
4. **OrquestraÃ§Ã£o** via AWS Lambda
5. **Processamento ETL** com AWS Glue
6. **Consultas analÃ­ticas** via AWS Athena

**Status Atual:** ğŸ‰ **Pipeline 100% Funcional** - Fases 1-3 concluÃ­das com sucesso

---

## ğŸ—ï¸ Estrutura do Projeto

```
bovespa-aws-pipeline/
â”œâ”€â”€ ğŸ“ .env                          # ConfiguraÃ§Ãµes AWS (nÃ£o versionado)
â”œâ”€â”€ ğŸ“ .env.template                 # Template de configuraÃ§Ã£o
â”œâ”€â”€ ğŸ“ backup_reset/                 # Backup dos componentes funcionais
â”œâ”€â”€ ğŸ“ config/                       # ConfiguraÃ§Ãµes YAML
â”‚   â”œâ”€â”€ app_config.yml               # ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
â”‚   â””â”€â”€ aws_config.yml               # ConfiguraÃ§Ãµes AWS
â”œâ”€â”€ ğŸ“ data/                         # Dados locais e processados
â”‚   â””â”€â”€ ğŸ“ processed/                # Arquivos Parquet processados
â”œâ”€â”€ ğŸ“ data_lake/                    # Estrutura local do data lake
â”œâ”€â”€ ğŸ“ docs/                         # DocumentaÃ§Ã£o do projeto
â”‚   â”œâ”€â”€ arquitetura_aws.md           # DocumentaÃ§Ã£o da arquitetura
â”‚   â”œâ”€â”€ kanban_de_progresso.md       # Status das tarefas
â”‚   â”œâ”€â”€ log_de_tarefas.md           # Log detalhado de atividades
â”‚   â””â”€â”€ diagrama_arquitetura.drawio  # Diagrama da soluÃ§Ã£o
â”œâ”€â”€ ğŸ“ infrastructure/               # Infraestrutura como cÃ³digo
â”‚   â”œâ”€â”€ ğŸ“ cloudformation/           # Templates CloudFormation
â”‚   â”‚   â”œâ”€â”€ main.yml                 # Stack principal
â”‚   â”‚   â””â”€â”€ README.md                # Guia de deploy
â”‚   â””â”€â”€ ğŸ“ scripts/                  # Scripts de deploy
â”‚       â””â”€â”€ deploy.sh                # Script de deployment
â”œâ”€â”€ ğŸ“ src/                          # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“ athena/                   # Consultas e views Athena
â”‚   â”‚   â”œâ”€â”€ ğŸ“ queries/              # Consultas analÃ­ticas
â”‚   â”‚   â”‚   â”œâ”€â”€ sector_analysis.sql   # AnÃ¡lise por setor
â”‚   â”‚   â”‚   â”œâ”€â”€ temporal_analysis.sql # AnÃ¡lise temporal
â”‚   â”‚   â”‚   â””â”€â”€ top_stocks.sql        # Top aÃ§Ãµes
â”‚   â”‚   â””â”€â”€ ğŸ“ views/                # Views materializadas
â”‚   â”‚       â”œâ”€â”€ sector_summary.sql    # Resumo por setor
â”‚   â”‚       â””â”€â”€ stock_ranking.sql     # Ranking de aÃ§Ãµes
â”‚   â”œâ”€â”€ ğŸ“ glue/                     # Jobs ETL Glue
â”‚   â”‚   â”œâ”€â”€ etl_job.py               # Job principal de ETL
â”‚   â”‚   â””â”€â”€ transformations.py       # TransformaÃ§Ãµes de dados
â”‚   â”œâ”€â”€ ğŸ“ lambda/                   # FunÃ§Ãµes Lambda
â”‚   â”‚   â”œâ”€â”€ requirements.txt         # DependÃªncias Lambda
â”‚   â”‚   â””â”€â”€ trigger_scraping.py      # Function principal
â”‚   â””â”€â”€ ğŸ“ scraping/                 # MÃ³dulo de coleta de dados
â”‚       â”œâ”€â”€ __init__.py              # InicializaÃ§Ã£o do mÃ³dulo
â”‚       â”œâ”€â”€ config.py                # ConfiguraÃ§Ãµes e URLs da B3
â”‚       â”œâ”€â”€ data_config.py           # Config de estrutura de dados
â”‚       â”œâ”€â”€ parquet_processor.py     # Processador Parquet
â”‚       â”œâ”€â”€ scraping.py              # Engine principal de scraping
â”‚       â””â”€â”€ utils.py                 # FunÃ§Ãµes auxiliares
â”œâ”€â”€ ğŸ“ tests/                        # Testes automatizados
â”‚   â”œâ”€â”€ __init__.py                  # InicializaÃ§Ã£o dos testes
â”‚   â”œâ”€â”€ test_etl.py                  # Testes do ETL Glue
â”‚   â”œâ”€â”€ test_lambda.py               # Testes da Lambda
â”‚   â”œâ”€â”€ test_s3_pipeline.py          # Testes do pipeline S3
â”‚   â””â”€â”€ test_scraping.py             # Testes do scraping
â”œâ”€â”€ test_pipeline_reset.py           # Script de teste completo
â”œâ”€â”€ pyproject.toml                   # ConfiguraÃ§Ã£o do projeto Python
â”œâ”€â”€ pytest.ini                      # ConfiguraÃ§Ã£o do pytest
â”œâ”€â”€ requirements.txt                 # DependÃªncias Python
â””â”€â”€ README.md                       # Este arquivo
```

---

## ğŸ›  Tecnologias e Ferramentas

### **Backend & Data Processing**
- **Python 3.9+** - Linguagem principal
- **Pandas 2.0+** - ManipulaÃ§Ã£o de dados
- **PyArrow 12.0+** - Processamento Parquet
- **Requests** - Cliente HTTP para APIs B3

### **AWS Cloud Services**
- **S3** - Data Lake (bucket: `bovespa-pipeline-data-adri-victor`)
- **Lambda** - OrquestraÃ§Ã£o e trigger do pipeline
- **Glue** - ETL visual e catÃ¡logo de dados
- **Athena** - Engine de consultas SQL
- **CloudFormation** - Infraestrutura como cÃ³digo

### **Desenvolvimento & Testes**
- **pytest** - Framework de testes (34/35 testes passando)
- **python-dotenv** - Gerenciamento de variÃ¡veis de ambiente
- **Git** - Controle de versÃ£o

---

## ğŸš€ Como Executar o Projeto

### **PrÃ©-requisitos**

1. **Python 3.9+ instalado**
2. **Conta AWS ativa** com permissÃµes para S3, Lambda, Glue e Athena
3. **AWS CLI configurado** (opcional, mas recomendado)
4. **Git** para clonar o repositÃ³rio

### **1. Setup Inicial**

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/goth-coder/bovespa-aws-pipeline.git
cd bovespa-aws-pipeline

# 2. Crie um ambiente virtual Python
python -m venv .venv

# 3. Ative o ambiente virtual
# No macOS/Linux:
source .venv/bin/activate
# No Windows:
# .venv\Scripts\activate

# 4. Instale as dependÃªncias
pip install -r requirements.txt
```

### **2. ConfiguraÃ§Ã£o de Credenciais AWS**

```bash
# 1. Copie o template de configuraÃ§Ã£o
cp .env.template .env

# 2. Edite o arquivo .env com suas credenciais AWS
# Abra .env em seu editor e configure:
```

**ConteÃºdo do arquivo `.env`:**
```bash
# AWS Credentials
AWS_ACCESS_KEY_ID=sua_access_key_aqui
AWS_SECRET_ACCESS_KEY=sua_secret_key_aqui
AWS_SESSION_TOKEN=seu_session_token_aqui  # (opcional, se usando MFA)
AWS_DEFAULT_REGION=us-east-1

# S3 Configuration
BOVESPA_S3_BUCKET=bovespa-pipeline-data-adri-victor
```

### **3. Teste Local do Pipeline**

```bash
# Execute o pipeline completo localmente
python test_pipeline_reset.py
```

**SaÃ­da esperada:**
```
âœ… Iniciando teste do pipeline resetado...
âœ… Coletando dados dos 4 endpoints da B3...
âœ… 339 aÃ§Ãµes coletadas com sucesso!
âœ… Processando arquivos JSON para Parquet...
âœ… 5 arquivos Parquet criados com 428 registros
âœ… Pipeline local executado com sucesso!
```

### **4. ExecuÃ§Ã£o dos Componentes Individuais**

#### **4.1. Scraping Isolado**
```bash
# Execute apenas o scraping
python -c "from src.scraping.scraping import run_scraping; run_scraping()"
```

#### **4.2. Processamento Parquet**
```bash
# Processe JSONs existentes para Parquet
python -c "from src.scraping.parquet_processor import ParquetProcessor; ParquetProcessor().process_all_json_files()"
```

#### **4.3. Testes Automatizados**
```bash
# Execute todos os testes
pytest -v

# Execute teste especÃ­fico
pytest tests/test_scraping.py -v
pytest tests/test_lambda.py -v
```

### **5. Deploy na AWS (Infraestrutura)**

```bash
# Execute o script de deploy CloudFormation
cd infrastructure/scripts
chmod +x deploy.sh
./deploy.sh
```

### **6. VerificaÃ§Ã£o de Dados no S3**

```bash
# Liste objetos no bucket S3 (requer AWS CLI)
aws s3 ls s3://bovespa-pipeline-data-adri-victor --recursive

# Estrutura esperada:
# dados/ano=2025/mes=08/dia=05/
```

---

## ğŸ“Š Status das Fases do Projeto

### **âœ… Fase 1: Coleta de Dados (CONCLUÃDA)**
- **ResponsÃ¡vel:** Victor (Agente A)
- **Status:** âœ… **100% Funcional**
- **Componentes:**
  - âœ… Scraping de 4 endpoints B3 (339 aÃ§Ãµes/execuÃ§Ã£o)
  - âœ… Estrutura modular em `src/scraping/`
  - âœ… ConfiguraÃ§Ã£o via `config.py` e `data_config.py`
  - âœ… Processamento JSON â†’ Parquet (428 registros)
  - âœ… Testes automatizados (13/13 passando)

### **âœ… Fase 2: Armazenamento S3 (CONCLUÃDA)**
- **ResponsÃ¡vel:** Victor (Agente A)
- **Status:** âœ… **100% Funcional**
- **Componentes:**
  - âœ… Bucket S3: `bovespa-pipeline-data-adri-victor`
  - âœ… Estrutura particionada: `ano=YYYY/mes=MM/dia=DD/`
  - âœ… Upload automÃ¡tico via `parquet_processor.py`
  - âœ… ConfiguraÃ§Ã£o via variÃ¡veis de ambiente (.env)

### **âœ… Fase 3: AWS Lambda (CONCLUÃDA)**
- **ResponsÃ¡vel:** Victor (Agente A)  
- **Status:** âœ… **100% Funcional**
- **Componentes:**
  - âœ… Function `trigger_scraping.py` (Status 200)
  - âœ… Handler completo: scraping â†’ parquet â†’ S3
  - âœ… Tratamento robusto de contexto (mock/AWS)
  - âœ… Testes funcionais (12/12 passando)

### **ğŸŸ¡ Fase 4: AWS Glue ETL (EM PROGRESSO)**
- **ResponsÃ¡vel:** Victor (Agente A)
- **Status:** ğŸŸ¡ **Estrutura Criada** 
- **Componentes:**
  - âœ… Job ETL base em `src/glue/etl_job.py`
  - âœ… TransformaÃ§Ãµes em `src/glue/transformations.py`
  - â³ Deploy e configuraÃ§Ã£o no Glue Studio
  - â³ IntegraÃ§Ã£o com Glue Catalog

### **ğŸŸ¡ Fase 5: Athena + AnÃ¡lise (EM PROGRESSO)**
- **ResponsÃ¡vel:** Adri (Agente B)
- **Status:** ğŸŸ¡ **Consultas Prontas**
- **Componentes:**
  - âœ… Consultas SQL em `src/athena/queries/`
  - âœ… Views materializadas em `src/athena/views/`
  - â³ ConfiguraÃ§Ã£o de tabelas externas
  - â³ Particionamento no Athena

### **ğŸ“‹ Fase 6: DocumentaÃ§Ã£o Final (PENDENTE)**
- **ResponsÃ¡vel:** Adri (Agente B)
- **Status:** â³ **Iniciado**
- **Componentes:**
  - âœ… README.md atualizado
  - âœ… DocumentaÃ§Ã£o tÃ©cnica
  - â³ VÃ­deo demonstrativo
  - â³ Guia de deployment completo

---

## ğŸ”§ Arquitetura do Pipeline

```mermaid
graph LR
    A[APIs B3] --> B[Scraping Python]
    B --> C[Parquet Files]
    C --> D[S3 Data Lake]
    D --> E[AWS Lambda]
    E --> F[AWS Glue ETL]
    F --> G[Glue Catalog]
    G --> H[AWS Athena]
    H --> I[Consultas SQL]
```

**Fluxo de Dados:**
1. **ExtraÃ§Ã£o:** APIs da B3 â†’ Scraping Python (339 aÃ§Ãµes)
2. **TransformaÃ§Ã£o:** JSON â†’ Parquet (428 registros)
3. **Carregamento:** Upload S3 com particionamento por data
4. **OrquestraÃ§Ã£o:** Lambda trigger automÃ¡tico
5. **Processamento:** AWS Glue ETL visual
6. **CatÃ¡logo:** Metadata no Glue Catalog
7. **Consulta:** Athena SQL para anÃ¡lises

---

## ğŸ“ˆ MÃ©tricas de Performance

- **ğŸ“Š Dados Coletados:** 339 aÃ§Ãµes por execuÃ§Ã£o
- **ğŸ“ Arquivos Gerados:** 5 Parquet files por execuÃ§Ã£o  
- **ğŸ“‹ Registros Processados:** 428 registros por batch
- **âœ… Taxa de Sucesso:** 97% (34/35 testes passando)
- **âš¡ Tempo de ExecuÃ§Ã£o:** ~30-45 segundos (pipeline completo)
- **ğŸ’¾ Tamanho MÃ©dio:** ~150KB por arquivo Parquet

---

## ğŸ›  Troubleshooting

### **Problemas Comuns**

#### **1. Erro de Credenciais AWS**
```bash
# Verifique se o arquivo .env estÃ¡ configurado corretamente
cat .env | grep AWS_

# Teste suas credenciais AWS
aws sts get-caller-identity
```

#### **2. Falha no Upload S3**
```bash
# Verifique se o bucket existe
aws s3 ls s3://bovespa-pipeline-data-adri-victor

# Verifique permissÃµes
aws s3api get-bucket-location --bucket bovespa-pipeline-data-adri-victor
```

#### **3. MÃ³dulos Python nÃ£o encontrados**
```bash
# Certifique-se de que o ambiente virtual estÃ¡ ativo
source .venv/bin/activate

# Reinstale as dependÃªncias
pip install -r requirements.txt
```

#### **4. Erro de API B3**
```bash
# Teste manualmente uma API da B3
curl "https://sistemaswebb3-listados.b3.com.br/indexProxy/indexCall/GetPortfolioDay/eyJsYW5ndWFnZSI6InB0LWJyIiwicGFnZU51bWJlciI6MSwicGFnZVNpemUiOjEyMCwiaW5kZXgiOiJJQk9WIiwic2VnbWVudCI6IjIifQ%3D%3D"
```

### **Logs e Debug**

```bash
# Visualize logs detalhados dos testes
pytest -v -s tests/

# Execute com debug no scraping
PYTHONPATH=/Users/adriannylelis/Workspace/bovespa-aws-pipeline python test_pipeline_reset.py
```

---

## ğŸ¤ ContribuiÃ§Ã£o

Este projeto estÃ¡ sendo desenvolvido como parte do **Tech Challenge - FIAP ML Engineering** em dupla:

- **ğŸ‘¤ Adrianny Lelis (Agente B)** - TransformaÃ§Ãµes, Athena, VisualizaÃ§Ã£o, DocumentaÃ§Ã£o
- **ğŸ‘¤ Victor Santos (Agente A)** - IngestÃ£o, Lambda, Glue ETL

### **Fluxo de Desenvolvimento**
1. Verifique o `docs/kanban_de_progresso.md` para status das tarefas
2. Registre atividades no `docs/log_de_tarefas.md`
3. FaÃ§a commits com identificaÃ§Ã£o do responsÃ¡vel
4. Mantenha documentaÃ§Ã£o atualizada

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ† CrÃ©ditos

**Autores:**
- Adrianny Lelis - [GitHub](https://github.com/goth-coder)
- Victor Santos - Parceiro de desenvolvimento

**InstituiÃ§Ã£o:** FIAP - Faculdade de InformÃ¡tica e AdministraÃ§Ã£o Paulista  
**Curso:** PÃ³s-graduaÃ§Ã£o em Machine Learning Engineering  
**Projeto:** Tech Challenge - Pipeline Batch Bovespa  
**PerÃ­odo:** 2025

---

## ğŸ“ Suporte

Para dÃºvidas ou problemas:

1. **Consulte a documentaÃ§Ã£o** em `docs/`
2. **Verifique os logs** em `docs/log_de_tarefas.md`
3. **Execute os testes** com `pytest -v`
4. **Abra uma issue** no GitHub (se aplicÃ¡vel)

---

*Ãšltima atualizaÃ§Ã£o: 05/08/2025*
- **ğŸ“ Arquivos Gerados:** 5 Parquet files por execuÃ§Ã£o  
- **ğŸ“‹ Registros Processados:** 428 registros por batch
- **âœ… Taxa de Sucesso:** 97% (34/35 testes passando)
- **âš¡ Tempo de ExecuÃ§Ã£o:** ~30-45 segundos (pipeline completo)
- **ğŸ’¾ Tamanho MÃ©dio:** ~150KB por arquivo Parquet

### **Processamento com AWS Glue**
- [ ] Criar Job ETL no modo visual
- [ ] Implementar transformaÃ§Ãµes obrigatÃ³rias:
  - [ ] Agrupamento numÃ©rico (sumarizaÃ§Ã£o, contagem ou soma)
  - [ ] Renomear 2 colunas
  - [ ] CÃ¡lculo com campos de data
- [ ] Salvar dados refinados no S3 `/refined/`
- [ ] Particionar por **data** e **nome/abreviaÃ§Ã£o da aÃ§Ã£o**
- [ ] Catalogar dados no Glue Catalog

### **Consulta e ValidaÃ§Ã£o**
- [ ] Consultar dados no AWS Athena
- [ ] Validar partiÃ§Ãµes e formato
- [ ] (Opcional) Criar Notebook no Athena com visualizaÃ§Ã£o grÃ¡fica

---

## ğŸ“‚ Estrutura do Projeto
```bash
bovespa-aws-pipeline/
â”œâ”€â”€ ğŸ“ scraping/           # Scripts de coleta de dados da B3
â”‚   â”œâ”€â”€ bovespa_scraper.py    # Scraper principal com conversÃ£o para Parquet
â”‚   â””â”€â”€ README.md             # DocumentaÃ§Ã£o do mÃ³dulo
â”‚
â”œâ”€â”€ ğŸ“ lambda/             # FunÃ§Ã£o AWS Lambda para orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ lambda_function.py    # Detecta S3 events e dispara Glue Job
â”‚   â”œâ”€â”€ requirements.txt      # DependÃªncias da Lambda
â”‚   â””â”€â”€ README.md             # DocumentaÃ§Ã£o da Lambda
â”‚
â”œâ”€â”€ ğŸ“ glue/               # ConfiguraÃ§Ãµes do AWS Glue ETL
â”‚   â”œâ”€â”€ glue_job_helper.py    # Helpers e configuraÃ§Ãµes do Job
â”‚   â””â”€â”€ README.md             # DocumentaÃ§Ã£o das transformaÃ§Ãµes
â”‚
â”œâ”€â”€ ğŸ“ config/             # ConfiguraÃ§Ãµes centralizadas
â”‚   â”œâ”€â”€ settings.py           # ConfiguraÃ§Ãµes Python
â”‚   â””â”€â”€ .env.example          # Exemplo de variÃ¡veis de ambiente
â”‚
â”œâ”€â”€ ğŸ“ scripts/            # Scripts de automaÃ§Ã£o e deploy
â”‚   â”œâ”€â”€ deploy_aws.py         # Deploy automÃ¡tico da infraestrutura
â”‚   â””â”€â”€ run_local_test.py     # Testes locais e scraping
â”‚
â”œâ”€â”€ ğŸ“ tests/              # Testes unitÃ¡rios e de integraÃ§Ã£o
â”‚   â”œâ”€â”€ test_scraping.py      # Testes do mÃ³dulo de scraping
â”‚   â””â”€â”€ README.md             # DocumentaÃ§Ã£o dos testes
â”‚
â”œâ”€â”€ ğŸ“ docs/               # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ architecture.md       # Arquitetura detalhada
â”‚   â””â”€â”€ deploy.md             # Guia completo de deploy
â”‚
â”œâ”€â”€ ğŸ“ infrastructure/     # Infrastructure as Code (Terraform)
â”‚   â””â”€â”€ main.tf               # DefiniÃ§Ã£o completa da infraestrutura AWS
â”‚
â”œâ”€â”€ ğŸ“„ QUICKSTART.md       # Guia de inÃ­cio rÃ¡pido
â”œâ”€â”€ ğŸ“„ pyproject.toml      # ConfiguraÃ§Ã£o do projeto Python
â”œâ”€â”€ ğŸ“„ requirements.txt    # DependÃªncias Python
â””â”€â”€ ğŸ“„ .gitignore          # Arquivos ignorados pelo Git
```
## ï¿½ Quick Start

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Configurar AWS
aws configure

# 3. Configurar projeto
cp config/.env.example config/.env

# 4. Testar localmente
python scripts/run_local_test.py --action test

# 5. Deploy da infraestrutura
cd infrastructure/ && terraform apply
```

ğŸ“– **Guia completo**: [QUICKSTART.md](QUICKSTART.md)

---
Fluxo esperado:
1. Scraping coleta dados da B3
2. Salva no S3 /raw/ em Parquet (partiÃ§Ã£o diÃ¡ria)
3. Evento S3 aciona AWS Lambda
4. Lambda dispara Job Glue
5. Glue processa dados e salva no S3 /refined/
6. Glue Catalog atualiza tabela
7. Consulta no Athena

## ğŸ“œ LicenÃ§a
Este projeto Ã© de uso acadÃªmico para o Tech Challenge de Big Data Architecture.