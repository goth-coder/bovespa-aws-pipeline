"""
AWS Glue ETL Job COMPLETO para transforma√ß√£o dos dados B3.
Implementa TODAS as transforma√ß√µes obrigat√≥rias do desafio:
1. Agrupamento num√©rico (sumariza√ß√£o, contagem ou soma)
2. Renomear 2 colunas
3. C√°lculo com campos de data
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import DataFrame
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# Inicializar contextos
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

def read_parquet_data(input_path: str) -> DataFrame:
    """
    L√™ dados Parquet do S3 e converte para DataFrame.
    """
    print(f"Lendo dados de: {input_path}")
    
    # Criar DynamicFrame a partir dos dados Parquet no S3
    dynamic_frame = glueContext.create_dynamic_frame.from_options(
        connection_type="s3",
        connection_options={
            "paths": [input_path],
            "recurse": True
        },
        format="parquet",
        format_options={}
    )
    
    # Converter para DataFrame do Spark
    df = dynamic_frame.toDF()
    
    print(f"DataFrame criado com {df.count()} registros")
    print(f"Schema: {df.printSchema()}")
    return df

def apply_transformations(df: DataFrame) -> DataFrame:
    """
    Aplica transforma√ß√µes obrigat√≥rias do desafio:
    ‚úÖ 1. Agrupamento num√©rico (sumariza√ß√£o, contagem ou soma)
    ‚úÖ 2. Renomear 2 colunas  
    ‚úÖ 3. C√°lculo com campos de data
    """
    print("üîÑ Aplicando transforma√ß√µes obrigat√≥rias...")
    
    # ‚úÖ TRANSFORMA√á√ÉO 1: Renomear 2 colunas (OBRIGAT√ìRIO)
    print("1Ô∏è‚É£ Renomeando colunas...")
    df = df.withColumnRenamed("codigo", "stock_code") \
           .withColumnRenamed("acao", "company_name")
    
    # ‚úÖ TRANSFORMA√á√ÉO 2: C√°lculo com campos de data (OBRIGAT√ìRIO)
    print("2Ô∏è‚É£ Adicionando c√°lculos de data...")
    df = df.withColumn("extraction_date", current_date()) \
           .withColumn("processing_timestamp", current_timestamp()) \
           .withColumn("year_month", date_format(current_date(), "yyyy-MM")) \
           .withColumn("quarter", quarter(current_date())) \
           .withColumn("days_since_extraction", 
                      datediff(current_date(), to_date(col("extraction_date"))))
    
    # ‚úÖ TRANSFORMA√á√ÉO 3: Agrupamento num√©rico (OBRIGAT√ìRIO)
    print("3Ô∏è‚É£ Criando agrega√ß√µes por setor...")
    
    # Dados individuais com ranking
    df_individual = df.withColumn("sector_rank", 
                                 row_number().over(
                                     Window.partitionBy("setor")
                                           .orderBy(col("part_percent").cast("double").desc())
                                 )) \
                     .withColumn("part_percent_double", col("part_percent").cast("double"))
    
    # Dados agregados por setor (sumariza√ß√£o, contagem, soma)
    df_sector_summary = df.groupBy("setor", "year_month", "quarter") \
                         .agg(
                             count("stock_code").alias("total_stocks_count"),
                             sum(col("part_percent").cast("double")).alias("total_sector_participation"),
                             avg(col("part_percent").cast("double")).alias("avg_sector_participation"),
                             max(col("part_percent").cast("double")).alias("max_sector_participation"),
                             min(col("part_percent").cast("double")).alias("min_sector_participation"),
                             stddev(col("part_percent").cast("double")).alias("stddev_sector_participation")
                         ) \
                         .withColumn("processing_timestamp", current_timestamp())
    
    print("‚úÖ Transforma√ß√µes aplicadas com sucesso!")
    print(f"üìä Registros individuais: {df_individual.count()}")
    print(f"üìà Setores agregados: {df_sector_summary.count()}")
    
    return df_individual, df_sector_summary

def write_parquet_partitioned(df_individual: DataFrame, df_summary: DataFrame, output_path: str):
    """
    Salva dados refinados em Parquet particionado por data e setor.
    """
    print(f"üíæ Salvando dados refinados em: {output_path}")
    
    # Salvar dados individuais particionados por ano/m√™s/setor
    individual_path = f"{output_path}/individual_stocks/"
    print(f"Salvando dados individuais em: {individual_path}")
    
    df_individual.write \
                 .mode("overwrite") \
                 .partitionBy("year_month", "setor") \
                 .option("compression", "snappy") \
                 .parquet(individual_path)
    
    # Salvar dados agregados particionados por ano/m√™s
    summary_path = f"{output_path}/sector_summary/"
    print(f"Salvando dados agregados em: {summary_path}")
    
    df_summary.write \
              .mode("overwrite") \
              .partitionBy("year_month", "quarter") \
              .option("compression", "snappy") \
              .parquet(summary_path)
    
    print("‚úÖ Dados salvos com sucesso!")

def register_glue_catalog(output_path: str):
    """
    Registra tabelas no Glue Catalog para uso no Athena.
    """
    print("üìã Registrando tabelas no Glue Catalog...")
    
    try:
        # Registrar tabela de dados individuais
        individual_path = f"{output_path}/individual_stocks/"
        individual_sink = glueContext.getSink(
            path=individual_path,
            connection_type="s3",
            updateBehavior="UPDATE_IN_DATABASE",
            partitionKeys=["year_month", "setor"],
            compression="snappy",
            enableUpdateCatalog=True,
            updateCatalogOptions={
                "database": "bovespa_db", 
                "tableName": "bovespa_individual_refined"
            }
        )
        
        # Registrar tabela de resumo por setor  
        summary_path = f"{output_path}/sector_summary/"
        summary_sink = glueContext.getSink(
            path=summary_path,
            connection_type="s3",
            updateBehavior="UPDATE_IN_DATABASE",
            partitionKeys=["year_month", "quarter"],
            compression="snappy",
            enableUpdateCatalog=True,
            updateCatalogOptions={
                "database": "bovespa_db", 
                "tableName": "bovespa_sector_summary"
            }
        )
        
        print("‚úÖ Tabelas registradas no Glue Catalog!")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao registrar no Glue Catalog: {e}")
        print("Continuando sem registro no cat√°logo...")

def main():
    """
    Fun√ß√£o principal do job ETL.
    """
    try:
        print("üöÄ Iniciando job ETL B3...")
        
        # Configura√ß√µes do job
        input_path = "s3://bovespa-pipeline-data-adri-vic/data_lake/"
        output_path = "s3://bovespa-pipeline-data-adri-vic/refined/"
        
        # 1. Ler dados brutos do S3
        print("üìñ Lendo dados do S3...")
        df = read_parquet_data(input_path)
        
        # 2. Aplicar transforma√ß√µes obrigat√≥rias
        print("üîß Aplicando transforma√ß√µes...")
        df_individual, df_summary = apply_transformations(df)
        
        # 3. Salvar dados refinados com particionamento
        print("üíæ Salvando dados refinados...")
        write_parquet_partitioned(df_individual, df_summary, output_path)
        
        # 4. Registrar no Glue Catalog
        register_glue_catalog(output_path)
        
        print("üéâ Job ETL B3 conclu√≠do com sucesso!")
        
    except Exception as e:
        print(f"‚ùå Erro no job ETL: {str(e)}")
        raise e
    finally:
        job.commit()

if __name__ == "__main__":
    main()
